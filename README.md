# ğŸ“Œ Configurable Training Pipeline with Checkpointing

## ğŸ“– Project Description
This project implements a configurable training pipeline using PyTorch.
Training parameters and file paths are managed through a YAML configuration file.
The pipeline supports checkpoint saving and resuming and logs training metrics to a CSV file for analysis.

## ğŸ›  Technologies Used
- Python
- PyTorch
- PyYAML
- CSV
- Google Colab

## ğŸ“‚ Project Structure
â”œâ”€â”€ Configurable_Training_Pipeline.ipynb
â”œâ”€â”€ config.yaml
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ model_epoch_X.pth
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ metrics.csv
â””â”€â”€ README.md

## âš™ï¸ Configuration File (config.yaml)
The YAML file is used to configure:
Number of epochs
Batch size
Learning rate
Checkpoint directory
Log directory
Resume checkpoint path

To resume training, update: 
```yaml
resume_checkpoint: checkpoints/model_epoch_3.pth

â–¶ï¸ How to Run
- Open the notebook in Google Colab
- Enable GPU:
         Runtime â†’ Change runtime type â†’ GPU (T4)
- Run all cells sequentially from top to bottom
- Training progress is printed per epoch
- Checkpoints are saved after every epoch
- Metrics are logged in logs/metrics.csv

ğŸ’¾ Checkpointing
- Model checkpoints are saved after each epoch
- Training can be resumed from the last saved checkpoint using config.yaml

ğŸ“Š Metrics Logging
- Training loss and accuracy are stored in metrics.csv
- This file can be used for later analysis or plotting

âœ… Output
- Console output showing epoch-wise loss and accuracy
- Saved model checkpoints
- CSV file containing training metrics
